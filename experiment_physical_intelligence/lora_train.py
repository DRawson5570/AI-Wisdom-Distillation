#!/usr/bin/env python3
"""
LoRA training helper for the Physical Intelligence experiment.

Features:
- Converts `student_training/curriculum.json` into HF-style JSONL train/val files
- Has a 'simulate' (dry-run) mode that validates data and prints stats
- Contains an example training function using Hugging Face Transformers + PEFT
  (commented / guarded) that you can enable if you have the right environment.

Usage (dry-run):
  python3 lora_train.py --simulate

Usage (prepare data):
  python3 lora_train.py --prepare

Usage (attempt training - requires heavy deps & GPU):
  python3 lora_train.py --train --model facebook/opt-125m --output-dir ./lora_output --epochs 3

Notes:
- The script expects `student_training/curriculum.json` to exist (generated by `run_student_training`).
- If you want to run actual LoRA fine-tuning, install:
    pip install transformers accelerate peft datasets bitsandbytes
  (requirements vary based on target model and hardware)

"""

import argparse
import json
import os
import random
from pathlib import Path

ROOT = Path(__file__).parent
CURRICULUM = ROOT / 'student_training' / 'curriculum_expanded.json'  # Use expanded curriculum
OUT_DIR = ROOT / 'student_training' / 'lora_data'


def prepare_jsonl(curriculum_path: Path, out_dir: Path, val_frac: float = 0.0):
    """Convert curriculum (list of {prompt,completion,scenario_id}) to train/validation JSONL files.
    Each line is: {"prompt": "...", "completion": "..."}
    
    Note: With only 30 examples, we skip validation split and use all data for training.
    Returns (train_file, val_file) where val_file may be None
    """
    assert curriculum_path.exists(), f"Curriculum not found: {curriculum_path}"
    with open(curriculum_path, 'r') as f:
        examples = json.load(f)

    random.seed(42)
    random.shuffle(examples)

    # With only 30 examples, use all for training
    if val_frac == 0.0 or len(examples) < 50:
        train = examples
        val = []
        print(f"âš ï¸  Small dataset ({len(examples)} examples) - using all for training, no validation split")
    else:
        n = len(examples)
        n_val = max(1, int(n * val_frac))
        val = examples[:n_val]
        train = examples[n_val:]

    out_dir.mkdir(parents=True, exist_ok=True)
    train_file = out_dir / "train.jsonl"
    val_file = out_dir / "val.jsonl" if val else None

    def write_jsonl(items, path: Path):
        with open(path, 'w') as fw:
            for ex in items:
                # Ensure we only include prompt and completion
                prompt = ex.get('prompt', '')
                completion = ex.get('completion', '')
                # Make sure completion is a short string suitable for causal LM targets
                fw.write(json.dumps({"prompt": prompt, "completion": completion}) + "\n")

    write_jsonl(train, train_file)
    if val:
        write_jsonl(val, val_file)

    print(f"âœ… Prepared {len(train)} train" + (f" and {len(val)} val examples" if val else " examples (no validation)"))
    print(f"  Train file: {train_file}")
    if val_file:
        print(f"  Val file:   {val_file}")
    return train_file, val_file


def simulate(curriculum_path: Path):
    """Quick checks and stats for the curriculum"""
    if not curriculum_path.exists():
        print(f"âŒ Curriculum missing: {curriculum_path}")
        return 1

    with open(curriculum_path, 'r') as f:
        curriculum = json.load(f)

    print(f"Loaded curriculum with {len(curriculum)} examples")

    # Count distributions of failure modes in completions
    freq = {}
    for ex in curriculum:
        comp = ex.get('completion', '').upper()
        for mode in ["CRUSHING", "DROPPING", "SLIPPING", "TIPPING", "ROLLING", "SLOSHING", "COLLISION"]:
            if mode in comp:
                freq[mode] = freq.get(mode, 0) + 1

    print("Failure mode frequency in curriculum (sample):")
    for k, v in sorted(freq.items(), key=lambda x: -x[1]):
        print(f"  {k}: {v}")

    # Print a short sample
    print("\nSample example (first):")
    print(json.dumps({"prompt": curriculum[0].get('prompt','')[:400], "completion": curriculum[0].get('completion','')[:400]}, indent=2))
    return 0


def train_with_peft(train_file: Path, val_file: Path, model_name: str, output_dir: Path, epochs: int = 3, batch_size: int = 8):
    """
    Example training function using Hugging Face transformers + PEFT.

    NOTE: This is a template. Running it requires proper environment, CUDA, and enough disk.
    Use at your own risk. If you only want to simulate, skip calling this.
    """
    try:
        import torch
        from datasets import load_dataset
        from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
        from peft import get_peft_model, LoraConfig, TaskType
    except Exception as e:
        print("âŒ Missing optional dependencies. Install `transformers datasets peft` to enable training.")
        print(e)
        return

    output_dir.mkdir(parents=True, exist_ok=True)

    # Load dataset
    ds_train = load_dataset('json', data_files=str(train_file))['train']
    ds_val = load_dataset('json', data_files=str(val_file))['train'] if val_file and val_file.exists() else None

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    def pack_examples(example):
        prompt = example['prompt']
        completion = example['completion']
        input_text = prompt + "\n\n" + completion
        tokenized = tokenizer(input_text, truncation=True, max_length=512, padding='max_length')
        # For causal LM, labels = input_ids (shifted internally by the model)
        tokenized['labels'] = tokenized['input_ids'].copy()
        return tokenized

    ds_train = ds_train.map(lambda ex: pack_examples(ex), batched=False, remove_columns=['prompt', 'completion'])
    if ds_val:
        ds_val = ds_val.map(lambda ex: pack_examples(ex), batched=False, remove_columns=['prompt', 'completion'])

    model = AutoModelForCausalLM.from_pretrained(
        model_name, 
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map='auto',
        low_cpu_mem_usage=True
    )
    
    # Enable gradient checkpointing to save memory
    model.gradient_checkpointing_enable()

    # Configure LoRA
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"]  # Target attention layers
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()  # Show what's being trained

    training_args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        eval_strategy='epoch' if ds_val else 'no',  # Updated from evaluation_strategy
        save_strategy='epoch',
        num_train_epochs=epochs,
        save_total_limit=2,
        learning_rate=3e-4,  # Critical: specify learning rate
        warmup_steps=10,
        logging_steps=1,
        logging_dir=str(output_dir / 'logs'),
        report_to='none',
        remove_unused_columns=False,  # Important for our custom data format
        fp16=True,  # Use mixed precision to save memory
        gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch
        gradient_checkpointing=True  # Enable gradient checkpointing
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds_train,
        eval_dataset=ds_val if ds_val else None,
        tokenizer=tokenizer
    )

    trainer.train()
    
    # Save the final LoRA adapter (not just checkpoints)
    print("ðŸ’¾ Saving final LoRA adapter...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    model.push_to_hub = False
    print(f"âœ… Training complete. LoRA weights saved to {output_dir}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--simulate', action='store_true', help='Run a dry-run analysis of curriculum')
    parser.add_argument('--prepare', action='store_true', help='Prepare JSONL train/val from curriculum')
    parser.add_argument('--train', action='store_true', help='Run LoRA training (requires deps)')
    parser.add_argument('--model', type=str, default='facebook/opt-125m', help='Base model for training (HF model id)')
    parser.add_argument('--output-dir', type=str, default=str(ROOT / 'student_training' / 'lora_output'), help='Output dir for LoRA weights')
    parser.add_argument('--epochs', type=int, default=3)
    parser.add_argument('--batch-size', type=int, default=8)
    args = parser.parse_args()

    if args.simulate:
        return simulate(CURRICULUM)

    if args.prepare:
        prepare_jsonl(CURRICULUM, OUT_DIR, val_frac=0.0)  # No validation split for small dataset
        return 0

    if args.train:
        val_file_path = OUT_DIR / 'val.jsonl'
        train_with_peft(
            OUT_DIR / 'train.jsonl', 
            val_file_path if val_file_path.exists() else None,
            args.model, 
            Path(args.output_dir), 
            epochs=args.epochs, 
            batch_size=args.batch_size
        )
        return 0

    parser.print_help()
    return 0


if __name__ == '__main__':
    main()
