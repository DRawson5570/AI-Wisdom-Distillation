{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9696969696969697,
  "eval_steps": 500,
  "global_step": 147,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020202020202020204,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 9.9651,
      "step": 1
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 9.9995,
      "step": 2
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 9.8363,
      "step": 3
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 49.530662536621094,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 9.8858,
      "step": 4
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 55.43154525756836,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 9.8745,
      "step": 5
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 58.49273681640625,
      "learning_rate": 8.999999999999999e-05,
      "loss": 9.8043,
      "step": 6
    },
    {
      "epoch": 0.1414141414141414,
      "grad_norm": 62.990535736083984,
      "learning_rate": 0.00011999999999999999,
      "loss": 9.1989,
      "step": 7
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 76.83201599121094,
      "learning_rate": 0.00015,
      "loss": 8.9248,
      "step": 8
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 83.92243194580078,
      "learning_rate": 0.00017999999999999998,
      "loss": 7.7775,
      "step": 9
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 83.17760467529297,
      "learning_rate": 0.00020999999999999998,
      "loss": 6.638,
      "step": 10
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 84.25656127929688,
      "learning_rate": 0.00023999999999999998,
      "loss": 5.2903,
      "step": 11
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 69.28901672363281,
      "learning_rate": 0.00027,
      "loss": 3.8722,
      "step": 12
    },
    {
      "epoch": 0.26262626262626265,
      "grad_norm": 58.28899002075195,
      "learning_rate": 0.0003,
      "loss": 2.7552,
      "step": 13
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 55.83745193481445,
      "learning_rate": 0.00029781021897810217,
      "loss": 2.1386,
      "step": 14
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 52.8745002746582,
      "learning_rate": 0.00029562043795620436,
      "loss": 2.0006,
      "step": 15
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 24.86934471130371,
      "learning_rate": 0.00029343065693430656,
      "loss": 1.7742,
      "step": 16
    },
    {
      "epoch": 0.3434343434343434,
      "grad_norm": 9.131268501281738,
      "learning_rate": 0.00029124087591240875,
      "loss": 1.6149,
      "step": 17
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 4.377187728881836,
      "learning_rate": 0.00028905109489051094,
      "loss": 1.5331,
      "step": 18
    },
    {
      "epoch": 0.3838383838383838,
      "grad_norm": 2.254770517349243,
      "learning_rate": 0.00028686131386861314,
      "loss": 1.4764,
      "step": 19
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 1.6276224851608276,
      "learning_rate": 0.0002846715328467153,
      "loss": 1.4443,
      "step": 20
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 1.3998844623565674,
      "learning_rate": 0.00028248175182481747,
      "loss": 1.4201,
      "step": 21
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.3765473365783691,
      "learning_rate": 0.00028029197080291966,
      "loss": 1.3481,
      "step": 22
    },
    {
      "epoch": 0.46464646464646464,
      "grad_norm": 1.2929736375808716,
      "learning_rate": 0.00027810218978102186,
      "loss": 1.293,
      "step": 23
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.3169987201690674,
      "learning_rate": 0.00027591240875912405,
      "loss": 1.2883,
      "step": 24
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 1.2850450277328491,
      "learning_rate": 0.00027372262773722625,
      "loss": 1.1888,
      "step": 25
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 1.3555874824523926,
      "learning_rate": 0.00027153284671532844,
      "loss": 1.1561,
      "step": 26
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 1.4295839071273804,
      "learning_rate": 0.00026934306569343063,
      "loss": 1.0843,
      "step": 27
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 1.4649053812026978,
      "learning_rate": 0.00026715328467153283,
      "loss": 1.0477,
      "step": 28
    },
    {
      "epoch": 0.5858585858585859,
      "grad_norm": 1.6968189477920532,
      "learning_rate": 0.000264963503649635,
      "loss": 1.0181,
      "step": 29
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 2.370253801345825,
      "learning_rate": 0.0002627737226277372,
      "loss": 0.9477,
      "step": 30
    },
    {
      "epoch": 0.6262626262626263,
      "grad_norm": 1.4330371618270874,
      "learning_rate": 0.0002605839416058394,
      "loss": 0.8908,
      "step": 31
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 1.9731736183166504,
      "learning_rate": 0.00025839416058394155,
      "loss": 0.8482,
      "step": 32
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.1484203338623047,
      "learning_rate": 0.00025620437956204374,
      "loss": 0.7815,
      "step": 33
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 1.5829428434371948,
      "learning_rate": 0.00025401459854014594,
      "loss": 0.7334,
      "step": 34
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 1.9917935132980347,
      "learning_rate": 0.00025182481751824813,
      "loss": 0.713,
      "step": 35
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.7583749294281006,
      "learning_rate": 0.0002496350364963503,
      "loss": 0.6836,
      "step": 36
    },
    {
      "epoch": 0.7474747474747475,
      "grad_norm": 1.475111484527588,
      "learning_rate": 0.0002474452554744525,
      "loss": 0.6004,
      "step": 37
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 1.7237658500671387,
      "learning_rate": 0.0002452554744525547,
      "loss": 0.5741,
      "step": 38
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 1.9407223463058472,
      "learning_rate": 0.0002430656934306569,
      "loss": 0.5161,
      "step": 39
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 2.3338754177093506,
      "learning_rate": 0.0002408759124087591,
      "loss": 0.4945,
      "step": 40
    },
    {
      "epoch": 0.8282828282828283,
      "grad_norm": 2.9179611206054688,
      "learning_rate": 0.0002386861313868613,
      "loss": 0.4382,
      "step": 41
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 3.7653727531433105,
      "learning_rate": 0.0002364963503649635,
      "loss": 0.4597,
      "step": 42
    },
    {
      "epoch": 0.8686868686868687,
      "grad_norm": 1.5450122356414795,
      "learning_rate": 0.00023430656934306568,
      "loss": 0.4166,
      "step": 43
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.3669614791870117,
      "learning_rate": 0.00023211678832116788,
      "loss": 0.3914,
      "step": 44
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 1.5239477157592773,
      "learning_rate": 0.00022992700729927004,
      "loss": 0.3523,
      "step": 45
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 1.403340220451355,
      "learning_rate": 0.00022773722627737224,
      "loss": 0.3503,
      "step": 46
    },
    {
      "epoch": 0.9494949494949495,
      "grad_norm": 1.1454066038131714,
      "learning_rate": 0.00022554744525547443,
      "loss": 0.3252,
      "step": 47
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.4638906717300415,
      "learning_rate": 0.00022335766423357663,
      "loss": 0.3123,
      "step": 48
    },
    {
      "epoch": 0.98989898989899,
      "grad_norm": 2.3554768562316895,
      "learning_rate": 0.00022116788321167882,
      "loss": 0.2865,
      "step": 49
    },
    {
      "epoch": 0.98989898989899,
      "eval_loss": 0.49133095145225525,
      "eval_runtime": 0.1084,
      "eval_samples_per_second": 27.685,
      "eval_steps_per_second": 27.685,
      "step": 49
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 2.334319591522217,
      "learning_rate": 0.00021897810218978101,
      "loss": 0.4163,
      "step": 50
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 2.8189470767974854,
      "learning_rate": 0.0002167883211678832,
      "loss": 0.3053,
      "step": 51
    },
    {
      "epoch": 1.0505050505050506,
      "grad_norm": 1.574413537979126,
      "learning_rate": 0.00021459854014598537,
      "loss": 0.2487,
      "step": 52
    },
    {
      "epoch": 1.0707070707070707,
      "grad_norm": 1.5246707201004028,
      "learning_rate": 0.00021240875912408757,
      "loss": 0.3056,
      "step": 53
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 1.7485430240631104,
      "learning_rate": 0.00021021897810218976,
      "loss": 0.2288,
      "step": 54
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.8796147108078003,
      "learning_rate": 0.00020802919708029196,
      "loss": 0.2413,
      "step": 55
    },
    {
      "epoch": 1.1313131313131313,
      "grad_norm": 1.080578327178955,
      "learning_rate": 0.00020583941605839415,
      "loss": 0.2386,
      "step": 56
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 1.0947816371917725,
      "learning_rate": 0.00020364963503649632,
      "loss": 0.2031,
      "step": 57
    },
    {
      "epoch": 1.1717171717171717,
      "grad_norm": 2.6278388500213623,
      "learning_rate": 0.0002014598540145985,
      "loss": 0.2433,
      "step": 58
    },
    {
      "epoch": 1.1919191919191918,
      "grad_norm": 1.7840116024017334,
      "learning_rate": 0.0001992700729927007,
      "loss": 0.2331,
      "step": 59
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.6994370222091675,
      "learning_rate": 0.0001970802919708029,
      "loss": 0.1787,
      "step": 60
    },
    {
      "epoch": 1.2323232323232323,
      "grad_norm": 1.080399990081787,
      "learning_rate": 0.0001948905109489051,
      "loss": 0.2362,
      "step": 61
    },
    {
      "epoch": 1.2525252525252526,
      "grad_norm": 1.1571049690246582,
      "learning_rate": 0.0001927007299270073,
      "loss": 0.2033,
      "step": 62
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 1.1759581565856934,
      "learning_rate": 0.00019051094890510948,
      "loss": 0.1759,
      "step": 63
    },
    {
      "epoch": 1.2929292929292928,
      "grad_norm": 0.9167013168334961,
      "learning_rate": 0.00018832116788321167,
      "loss": 0.1655,
      "step": 64
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 1.0783442258834839,
      "learning_rate": 0.00018613138686131387,
      "loss": 0.1779,
      "step": 65
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.8628808259963989,
      "learning_rate": 0.00018394160583941606,
      "loss": 0.1455,
      "step": 66
    },
    {
      "epoch": 1.3535353535353536,
      "grad_norm": 0.9705929160118103,
      "learning_rate": 0.00018175182481751826,
      "loss": 0.1454,
      "step": 67
    },
    {
      "epoch": 1.3737373737373737,
      "grad_norm": 1.2337844371795654,
      "learning_rate": 0.00017956204379562042,
      "loss": 0.1444,
      "step": 68
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 1.273052453994751,
      "learning_rate": 0.00017737226277372262,
      "loss": 0.1607,
      "step": 69
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 1.3333789110183716,
      "learning_rate": 0.00017518248175182478,
      "loss": 0.1569,
      "step": 70
    },
    {
      "epoch": 1.4343434343434343,
      "grad_norm": 0.6994220018386841,
      "learning_rate": 0.00017299270072992698,
      "loss": 0.1393,
      "step": 71
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.8661051392555237,
      "learning_rate": 0.00017080291970802917,
      "loss": 0.1545,
      "step": 72
    },
    {
      "epoch": 1.4747474747474747,
      "grad_norm": 1.094193696975708,
      "learning_rate": 0.00016861313868613137,
      "loss": 0.1273,
      "step": 73
    },
    {
      "epoch": 1.494949494949495,
      "grad_norm": 1.06472909450531,
      "learning_rate": 0.00016642335766423356,
      "loss": 0.163,
      "step": 74
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.9714722037315369,
      "learning_rate": 0.00016423357664233575,
      "loss": 0.164,
      "step": 75
    },
    {
      "epoch": 1.5353535353535355,
      "grad_norm": 0.9061802625656128,
      "learning_rate": 0.00016204379562043795,
      "loss": 0.1993,
      "step": 76
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.8219131231307983,
      "learning_rate": 0.00015985401459854014,
      "loss": 0.1356,
      "step": 77
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 1.1115409135818481,
      "learning_rate": 0.00015766423357664234,
      "loss": 0.1427,
      "step": 78
    },
    {
      "epoch": 1.595959595959596,
      "grad_norm": 0.99244225025177,
      "learning_rate": 0.00015547445255474453,
      "loss": 0.1352,
      "step": 79
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 1.0069233179092407,
      "learning_rate": 0.00015328467153284672,
      "loss": 0.1727,
      "step": 80
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.7253667116165161,
      "learning_rate": 0.00015109489051094892,
      "loss": 0.1228,
      "step": 81
    },
    {
      "epoch": 1.6565656565656566,
      "grad_norm": 1.1598888635635376,
      "learning_rate": 0.00014890510948905108,
      "loss": 0.1191,
      "step": 82
    },
    {
      "epoch": 1.676767676767677,
      "grad_norm": 0.8671983480453491,
      "learning_rate": 0.00014671532846715328,
      "loss": 0.1356,
      "step": 83
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.6784946322441101,
      "learning_rate": 0.00014452554744525547,
      "loss": 0.1137,
      "step": 84
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 0.6153172254562378,
      "learning_rate": 0.00014233576642335764,
      "loss": 0.1181,
      "step": 85
    },
    {
      "epoch": 1.7373737373737375,
      "grad_norm": 0.8076702952384949,
      "learning_rate": 0.00014014598540145983,
      "loss": 0.1148,
      "step": 86
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.6346065998077393,
      "learning_rate": 0.00013795620437956203,
      "loss": 0.1156,
      "step": 87
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.47304636240005493,
      "learning_rate": 0.00013576642335766422,
      "loss": 0.1108,
      "step": 88
    },
    {
      "epoch": 1.797979797979798,
      "grad_norm": 0.6149647235870361,
      "learning_rate": 0.00013357664233576641,
      "loss": 0.1125,
      "step": 89
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.5313335657119751,
      "learning_rate": 0.0001313868613138686,
      "loss": 0.1084,
      "step": 90
    },
    {
      "epoch": 1.8383838383838382,
      "grad_norm": 0.7491944432258606,
      "learning_rate": 0.00012919708029197077,
      "loss": 0.1268,
      "step": 91
    },
    {
      "epoch": 1.8585858585858586,
      "grad_norm": 0.9048624634742737,
      "learning_rate": 0.00012700729927007297,
      "loss": 0.1307,
      "step": 92
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.8641659617424011,
      "learning_rate": 0.00012481751824817516,
      "loss": 0.1076,
      "step": 93
    },
    {
      "epoch": 1.898989898989899,
      "grad_norm": 1.1251113414764404,
      "learning_rate": 0.00012262773722627736,
      "loss": 0.1453,
      "step": 94
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 0.6094756126403809,
      "learning_rate": 0.00012043795620437955,
      "loss": 0.1122,
      "step": 95
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.45631733536720276,
      "learning_rate": 0.00011824817518248174,
      "loss": 0.1116,
      "step": 96
    },
    {
      "epoch": 1.9595959595959596,
      "grad_norm": 0.7424740791320801,
      "learning_rate": 0.00011605839416058394,
      "loss": 0.1266,
      "step": 97
    },
    {
      "epoch": 1.9797979797979797,
      "grad_norm": 0.639911949634552,
      "learning_rate": 0.00011386861313868612,
      "loss": 0.0997,
      "step": 98
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7738196849822998,
      "learning_rate": 0.00011167883211678831,
      "loss": 0.1591,
      "step": 99
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.33499976992607117,
      "eval_runtime": 0.1054,
      "eval_samples_per_second": 28.464,
      "eval_steps_per_second": 28.464,
      "step": 99
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 0.6293169260025024,
      "learning_rate": 0.00010948905109489051,
      "loss": 0.1089,
      "step": 100
    },
    {
      "epoch": 2.04040404040404,
      "grad_norm": 0.4476003348827362,
      "learning_rate": 0.00010729927007299269,
      "loss": 0.1079,
      "step": 101
    },
    {
      "epoch": 2.0606060606060606,
      "grad_norm": 0.6636972427368164,
      "learning_rate": 0.00010510948905109488,
      "loss": 0.1187,
      "step": 102
    },
    {
      "epoch": 2.080808080808081,
      "grad_norm": 0.7158880233764648,
      "learning_rate": 0.00010291970802919708,
      "loss": 0.1346,
      "step": 103
    },
    {
      "epoch": 2.101010101010101,
      "grad_norm": 0.4569774568080902,
      "learning_rate": 0.00010072992700729926,
      "loss": 0.1009,
      "step": 104
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 0.5564879179000854,
      "learning_rate": 9.854014598540145e-05,
      "loss": 0.1096,
      "step": 105
    },
    {
      "epoch": 2.1414141414141414,
      "grad_norm": 0.49733927845954895,
      "learning_rate": 9.635036496350364e-05,
      "loss": 0.1041,
      "step": 106
    },
    {
      "epoch": 2.1616161616161618,
      "grad_norm": 0.4097895622253418,
      "learning_rate": 9.416058394160584e-05,
      "loss": 0.1016,
      "step": 107
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.9734423756599426,
      "learning_rate": 9.197080291970803e-05,
      "loss": 0.1325,
      "step": 108
    },
    {
      "epoch": 2.202020202020202,
      "grad_norm": 0.65766841173172,
      "learning_rate": 8.978102189781021e-05,
      "loss": 0.1111,
      "step": 109
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.5698248744010925,
      "learning_rate": 8.759124087591239e-05,
      "loss": 0.1209,
      "step": 110
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 0.5282817482948303,
      "learning_rate": 8.540145985401459e-05,
      "loss": 0.1098,
      "step": 111
    },
    {
      "epoch": 2.2626262626262625,
      "grad_norm": 0.7355577945709229,
      "learning_rate": 8.321167883211678e-05,
      "loss": 0.1149,
      "step": 112
    },
    {
      "epoch": 2.282828282828283,
      "grad_norm": 0.7278246283531189,
      "learning_rate": 8.102189781021897e-05,
      "loss": 0.1031,
      "step": 113
    },
    {
      "epoch": 2.303030303030303,
      "grad_norm": 0.47473278641700745,
      "learning_rate": 7.883211678832117e-05,
      "loss": 0.1078,
      "step": 114
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 0.5461296439170837,
      "learning_rate": 7.664233576642336e-05,
      "loss": 0.1047,
      "step": 115
    },
    {
      "epoch": 2.3434343434343434,
      "grad_norm": 0.5666427612304688,
      "learning_rate": 7.445255474452554e-05,
      "loss": 0.1014,
      "step": 116
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.849906325340271,
      "learning_rate": 7.226277372262774e-05,
      "loss": 0.1384,
      "step": 117
    },
    {
      "epoch": 2.3838383838383836,
      "grad_norm": 0.5685586929321289,
      "learning_rate": 7.007299270072992e-05,
      "loss": 0.1066,
      "step": 118
    },
    {
      "epoch": 2.404040404040404,
      "grad_norm": 0.35454830527305603,
      "learning_rate": 6.788321167883211e-05,
      "loss": 0.1078,
      "step": 119
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.6238660216331482,
      "learning_rate": 6.56934306569343e-05,
      "loss": 0.0964,
      "step": 120
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.7001282572746277,
      "learning_rate": 6.350364963503648e-05,
      "loss": 0.1229,
      "step": 121
    },
    {
      "epoch": 2.4646464646464645,
      "grad_norm": 0.8169247508049011,
      "learning_rate": 6.131386861313868e-05,
      "loss": 0.1232,
      "step": 122
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 0.5379732251167297,
      "learning_rate": 5.912408759124087e-05,
      "loss": 0.1023,
      "step": 123
    },
    {
      "epoch": 2.505050505050505,
      "grad_norm": 0.6134966015815735,
      "learning_rate": 5.693430656934306e-05,
      "loss": 0.1067,
      "step": 124
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 0.5330060124397278,
      "learning_rate": 5.4744525547445253e-05,
      "loss": 0.1044,
      "step": 125
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.5245463252067566,
      "learning_rate": 5.255474452554744e-05,
      "loss": 0.1,
      "step": 126
    },
    {
      "epoch": 2.5656565656565657,
      "grad_norm": 0.5458305478096008,
      "learning_rate": 5.036496350364963e-05,
      "loss": 0.0996,
      "step": 127
    },
    {
      "epoch": 2.5858585858585856,
      "grad_norm": 0.44204333424568176,
      "learning_rate": 4.817518248175182e-05,
      "loss": 0.1018,
      "step": 128
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 0.5906593799591064,
      "learning_rate": 4.5985401459854016e-05,
      "loss": 0.1193,
      "step": 129
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 0.5738992691040039,
      "learning_rate": 4.3795620437956196e-05,
      "loss": 0.1098,
      "step": 130
    },
    {
      "epoch": 2.6464646464646466,
      "grad_norm": 0.3923337459564209,
      "learning_rate": 4.160583941605839e-05,
      "loss": 0.1018,
      "step": 131
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.42341119050979614,
      "learning_rate": 3.9416058394160584e-05,
      "loss": 0.1067,
      "step": 132
    },
    {
      "epoch": 2.686868686868687,
      "grad_norm": 0.36744415760040283,
      "learning_rate": 3.722627737226277e-05,
      "loss": 0.0982,
      "step": 133
    },
    {
      "epoch": 2.707070707070707,
      "grad_norm": 0.4668113887310028,
      "learning_rate": 3.503649635036496e-05,
      "loss": 0.0933,
      "step": 134
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.4169374704360962,
      "learning_rate": 3.284671532846715e-05,
      "loss": 0.0916,
      "step": 135
    },
    {
      "epoch": 2.7474747474747474,
      "grad_norm": 0.5571342706680298,
      "learning_rate": 3.065693430656934e-05,
      "loss": 0.108,
      "step": 136
    },
    {
      "epoch": 2.7676767676767677,
      "grad_norm": 0.5624401569366455,
      "learning_rate": 2.846715328467153e-05,
      "loss": 0.107,
      "step": 137
    },
    {
      "epoch": 2.787878787878788,
      "grad_norm": 0.5504180788993835,
      "learning_rate": 2.627737226277372e-05,
      "loss": 0.0895,
      "step": 138
    },
    {
      "epoch": 2.808080808080808,
      "grad_norm": 0.45762792229652405,
      "learning_rate": 2.408759124087591e-05,
      "loss": 0.0955,
      "step": 139
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 0.7624447345733643,
      "learning_rate": 2.1897810218978098e-05,
      "loss": 0.0965,
      "step": 140
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 0.5140657424926758,
      "learning_rate": 1.9708029197080292e-05,
      "loss": 0.1,
      "step": 141
    },
    {
      "epoch": 2.8686868686868685,
      "grad_norm": 0.4220660328865051,
      "learning_rate": 1.751824817518248e-05,
      "loss": 0.0946,
      "step": 142
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.5235731601715088,
      "learning_rate": 1.532846715328467e-05,
      "loss": 0.0984,
      "step": 143
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.6306883692741394,
      "learning_rate": 1.313868613138686e-05,
      "loss": 0.1021,
      "step": 144
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 0.502862274646759,
      "learning_rate": 1.0948905109489049e-05,
      "loss": 0.0976,
      "step": 145
    },
    {
      "epoch": 2.9494949494949494,
      "grad_norm": 0.5367368459701538,
      "learning_rate": 8.75912408759124e-06,
      "loss": 0.1042,
      "step": 146
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 0.6421234011650085,
      "learning_rate": 6.56934306569343e-06,
      "loss": 0.1091,
      "step": 147
    },
    {
      "epoch": 2.9696969696969697,
      "eval_loss": 0.3197669982910156,
      "eval_runtime": 0.1154,
      "eval_samples_per_second": 26.004,
      "eval_steps_per_second": 26.004,
      "step": 147
    }
  ],
  "logging_steps": 1,
  "max_steps": 147,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 647460879335424.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
